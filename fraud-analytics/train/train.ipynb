{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "htTRXObCAAf5",
    "outputId": "e97eb66c-b22d-4b33-9216-90f7ebd7b76f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TabFormer'...\n",
      "remote: Enumerating objects: 114, done.\u001b[K\n",
      "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
      "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
      "remote: Total 114 (delta 11), reused 9 (delta 9), pack-reused 95 (from 1)\u001b[K\n",
      "Receiving objects: 100% (114/114), 452.37 KiB | 7.42 MiB/s, done.\n",
      "Resolving deltas: 100% (47/47), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/IBM/TabFormer.git && \\\n",
    "cd TabFormer && \\\n",
    "git lfs pull && \\\n",
    "tar xzf data/credit_card/transactions.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DWbb152LBv98",
    "outputId": "6c698cb6-e7ed-4466-9326-9863166b8059"
   },
   "outputs": [],
   "source": [
    "# Read and load the csv file\n",
    "df = pd.read_csv('./TabFormer/card_transaction.v1.csv')\n",
    "df = df[0:16500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fii14PLavArb",
    "outputId": "4ac93518-f624-49e1-f839-743caaea7841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16500000 entries, 0 to 16499999\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   User            int64  \n",
      " 1   Card            int64  \n",
      " 2   Year            int64  \n",
      " 3   Month           int64  \n",
      " 4   Day             int64  \n",
      " 5   Time            object \n",
      " 6   Amount          object \n",
      " 7   Use Chip        object \n",
      " 8   Merchant Name   object \n",
      " 9   Merchant City   object \n",
      " 10  Merchant State  object \n",
      " 11  Zip             float64\n",
      " 12  MCC             int64  \n",
      " 13  Errors?         object \n",
      " 14  Is Fraud?       object \n",
      "dtypes: float64(1), int64(6), object(8)\n",
      "memory usage: 1.8+ GB\n",
      "None\n",
      "16475254 8237627 4942576 3295051\n",
      "[10564891  4293011 12990080 ...  7946973  8363815  6839602] [ 5898230  4785713  4951019 ...  3062006  6616067 11640215] [       6        8       27 ... 16499980 16499986 16499993]\n"
     ]
    }
   ],
   "source": [
    "# Set sequence length for multivariate time series\n",
    "seq_length = 7\n",
    "\n",
    "df['Merchant Name'] = df['Merchant Name'].astype(str)\n",
    "df.sort_values(by=['User','Card'], inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "print (df.info())\n",
    "\n",
    "# Get first of each User-Card combination\n",
    "first = df[['User','Card']].drop_duplicates()\n",
    "f = np.array(first.index)\n",
    "\n",
    "# Drop the first N transactions\n",
    "drop_list = np.concatenate([np.arange(x,x + seq_length - 1) for x in f])\n",
    "index_list = np.setdiff1d(df.index.values,drop_list)\n",
    "\n",
    "# Split into 0.5 train, 0.3 validate, 0.2 test\n",
    "tot_length = index_list.shape[0]\n",
    "train_length = tot_length // 2\n",
    "validate_length = (tot_length - train_length) * 3 // 5\n",
    "test_length = tot_length - train_length - validate_length\n",
    "print (tot_length,train_length,validate_length, test_length)\n",
    "\n",
    "# Generate list of indices for train, validate, test\n",
    "np.random.seed(1111)\n",
    "train_indices = np.random.choice(index_list, train_length, replace=False)\n",
    "tv_list = np.setdiff1d(index_list, train_indices)\n",
    "validate_indices = np.random.choice(tv_list, validate_length, replace=False)\n",
    "test_indices = np.setdiff1d(tv_list, validate_indices)\n",
    "print(train_indices, validate_indices, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SbujKdzTxodG"
   },
   "outputs": [],
   "source": [
    "# ----- CUSTOM MAPPING FUNCTIONS -----\n",
    "def timeEncoder(X):\n",
    "    X_hm = X['Time'].str.split(':', expand=True)\n",
    "    d = pd.to_datetime(dict(year=X['Year'],month=X['Month'],day=X['Day'],hour=X_hm[0],minute=X_hm[1])).astype(int)\n",
    "    return pd.DataFrame(d)\n",
    "\n",
    "def amtEncoder(X):\n",
    "    amt = X.apply(lambda x: x[1:]).astype(float).map(lambda amt: max(1,amt)).map(math.log)\n",
    "    return pd.DataFrame(amt)\n",
    "\n",
    "def decimalEncoder(X,length=5):\n",
    "    dnew = pd.DataFrame()\n",
    "    for i in range(length):\n",
    "        dnew[i] = np.mod(X,10) \n",
    "        X = np.floor_divide(X,10)\n",
    "    return dnew\n",
    "\n",
    "def fraudEncoder(X):\n",
    "    return np.where(X == 'Yes', 1, 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_dir = \"/opt/artifacts/\"\n",
    "os.makedirs(artifact_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "ihdoXk8uOUts",
    "outputId": "4bf0116a-ef9d-4c04-fd3c-205ae9e50b19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib64/python3.12/site-packages/sklearn/utils/deprecation.py:95: FutureWarning: Function tosequence is deprecated; tosequence was deprecated in 1.5 and will be removed in 1.7\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "mapper = DataFrameMapper([('Is Fraud?', FunctionTransformer(fraudEncoder)),\n",
    "                          (['Merchant State'], [SimpleImputer(strategy='constant'), FunctionTransformer(np.ravel),\n",
    "                                               LabelEncoder(), FunctionTransformer(decimalEncoder), OneHotEncoder()]),\n",
    "                          (['Zip'], [SimpleImputer(strategy='constant'), FunctionTransformer(np.ravel),\n",
    "                                     FunctionTransformer(decimalEncoder), OneHotEncoder()]),\n",
    "                          ('Merchant Name', [LabelEncoder(), FunctionTransformer(decimalEncoder), OneHotEncoder()]),\n",
    "                          ('Merchant City', [LabelEncoder(), FunctionTransformer(decimalEncoder), OneHotEncoder()]),\n",
    "                          ('MCC', [LabelEncoder(), FunctionTransformer(decimalEncoder), OneHotEncoder()]),\n",
    "                          (['Use Chip'], [SimpleImputer(strategy='constant'), LabelBinarizer()]),\n",
    "                          (['Errors?'], [SimpleImputer(strategy='constant'), LabelBinarizer()]),\n",
    "                          (['Year','Month','Day','Time'], [FunctionTransformer(timeEncoder), MinMaxScaler()]),\n",
    "                          ('Amount', [FunctionTransformer(amtEncoder), MinMaxScaler()])\n",
    "                         ], input_df=True, df_out=True)\n",
    "mapper.fit(df)\n",
    "joblib.dump(mapper, open(os.path.join(artifact_dir, 'fitted_mapper.pkl'),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2RlYLJIy-TT7",
    "outputId": "05ae9466-6064-483a-ecde-44ca9081dc05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n"
     ]
    }
   ],
   "source": [
    "mapped_sample = mapper.transform(df[:100])\n",
    "mapped_size = mapped_sample.shape[-1]\n",
    "print(mapped_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AJEulvhGVZlR"
   },
   "outputs": [],
   "source": [
    "def gen_training_batch(df, mapper, index_list, batch_size):\n",
    "    np.random.seed(98765)\n",
    "    train_df = df.loc[index_list]\n",
    "    non_fraud_indices = train_df[train_df['Is Fraud?'] == 'No'].index.values\n",
    "    fraud_indices = train_df[train_df['Is Fraud?'] == 'Yes'].index.values\n",
    "    fsize = fraud_indices.shape[0]\n",
    "    while True:\n",
    "        indices = np.concatenate((fraud_indices,np.random.choice(non_fraud_indices,fsize,replace=False)))\n",
    "        np.random.shuffle(indices)\n",
    "        rows = indices.shape[0]\n",
    "        index_array = np.zeros((rows, seq_length), dtype=int)\n",
    "        for i in range(seq_length):\n",
    "            index_array[:,i] = indices + 1 - seq_length + i\n",
    "        full_df = mapper.transform(df.loc[index_array.flatten()])\n",
    "        target_buffer = full_df['Is Fraud?'].to_numpy().reshape(rows, seq_length, 1)\n",
    "        data_buffer = full_df.drop(['Is Fraud?'],axis=1).to_numpy().reshape(rows, seq_length, -1)\n",
    "\n",
    "        batch_ptr = 0\n",
    "        while (batch_ptr + batch_size) <= rows:\n",
    "            data = data_buffer[batch_ptr:batch_ptr+batch_size]\n",
    "            targets = target_buffer[batch_ptr:batch_ptr+batch_size]\n",
    "            batch_ptr += batch_size\n",
    "            data_t = np.transpose(data, axes=(1,0,2))\n",
    "            targets_t = np.transpose(targets, axes=(1,0,2))\n",
    "            yield data_t,targets_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2_WhrZC8Vad_"
   },
   "outputs": [],
   "source": [
    "class TP(tf.keras.metrics.TruePositives):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        super().update_state(y_true[-1,:,:], y_pred[-1,:,:], sample_weight)\n",
    "\n",
    "class FP(tf.keras.metrics.FalsePositives):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        super().update_state(y_true[-1,:,:], y_pred[-1,:,:], sample_weight)\n",
    "\n",
    "class FN(tf.keras.metrics.FalseNegatives):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        super().update_state(y_true[-1,:,:], y_pred[-1,:,:], sample_weight)\n",
    "\n",
    "class TN(tf.keras.metrics.TrueNegatives):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        super().update_state(y_true[-1,:,:], y_pred[-1,:,:], sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLefOsH2VG3D",
    "outputId": "1f8a005d-5fec-468a-e8c6-2a8046698ef7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib64/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">336,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">320,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">201</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m200\u001b[0m)         │       \u001b[38;5;34m336,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m200\u001b[0m)         │       \u001b[38;5;34m320,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │           \u001b[38;5;34m201\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">657,001</span> (2.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m657,001\u001b[0m (2.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">657,001</span> (2.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m657,001\u001b[0m (2.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "units = [200,200]\n",
    "input_size = mapped_size - 1\n",
    "output_size = 1\n",
    "\n",
    "tf_input = ([seq_length, input_size])\n",
    "\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(units[0], input_shape=tf_input, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(units[1], return_sequences=True),\n",
    "    tf.keras.layers.Dense(output_size, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.summary()\n",
    "\n",
    "metrics=['accuracy', \n",
    "    TP(name='TP'),\n",
    "    FP(name='FP'),\n",
    "    FN(name='FN'),\n",
    "    TN(name='TN'),\n",
    "    tf.keras.metrics.TruePositives(name='tp'),\n",
    "    tf.keras.metrics.FalsePositives(name='fp'),\n",
    "    tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "    tf.keras.metrics.TrueNegatives(name='tn')\n",
    "   ]\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HZ5DHKOXVrPd"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = 10000\n",
    "filepath = artifact_dir + \".weights.h5\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rFc-pSPAV2q9",
    "outputId": "010a7f2c-f5a1-4b89-8fab-aceb675ed4b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning...\n",
      "Epoch 1/5\n",
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - FN: 5662.1484 - FP: 1091.2725 - TN: 38903.0625 - TP: 34351.5156 - accuracy: 0.9437 - fn: 18776.6387 - fp: 7782.0859 - loss: 0.1569 - tn: 421590.7188 - tp: 111906.5625\n",
      "Epoch 1: saving model to /opt/artifacts/.weights.h5\n",
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 28ms/step - FN: 9148.0000 - FP: 2081.0000 - TN: 77923.0000 - TP: 70848.0000 - accuracy: 0.9599 - fn: 30356.0000 - fp: 14571.0000 - loss: 0.1073 - tn: 844198.0000 - tp: 230875.0000\n",
      "Epoch 2/5\n",
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - FN: 2069.2429 - FP: 864.9874 - TN: 39127.1719 - TP: 37946.5977 - accuracy: 0.9758 - fn: 7306.2383 - fp: 5973.7446 - loss: 0.0574 - tn: 423383.0000 - tp: 123393.0078\n",
      "Epoch 2: saving model to /opt/artifacts/.weights.h5\n",
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 28ms/step - FN: 3976.0000 - FP: 1683.0000 - TN: 78321.0000 - TP: 76020.0000 - accuracy: 0.9771 - fn: 14179.0000 - fp: 11515.0000 - loss: 0.0542 - tn: 847277.0000 - tp: 247029.0000\n",
      "Epoch 3/5\n",
      "\u001b[1m 9998/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - FN: 1664.2072 - FP: 725.0188 - TN: 39265.7148 - TP: 38337.0586 - accuracy: 0.9799 - fn: 6254.0654 - fp: 4845.2773 - loss: 0.0463 - tn: 424441.7812 - tp: 124402.8828\n",
      "Epoch 3: saving model to /opt/artifacts/.weights.h5\n",
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 28ms/step - FN: 3211.0000 - FP: 1455.0000 - TN: 78566.0000 - TP: 76768.0000 - accuracy: 0.9806 - fn: 12131.0000 - fp: 9593.0000 - loss: 0.0444 - tn: 849181.0000 - tp: 249095.0000\n",
      "Epoch 4/5\n",
      "\u001b[1m 9999/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - FN: 1382.5847 - FP: 706.1631 - TN: 39255.5312 - TP: 38655.7227 - accuracy: 0.9822 - fn: 5328.6494 - fp: 4647.0996 - loss: 0.0404 - tn: 424681.7188 - tp: 125342.5391\n",
      "Epoch 4: saving model to /opt/artifacts/.weights.h5\n",
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 28ms/step - FN: 2746.0000 - FP: 1416.0000 - TN: 78538.0000 - TP: 77300.0000 - accuracy: 0.9822 - fn: 10644.0000 - fp: 9285.0000 - loss: 0.0404 - tn: 849398.0000 - tp: 250673.0000\n",
      "Epoch 5/5\n",
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - FN: 1263.8988 - FP: 689.1372 - TN: 39321.1328 - TP: 38733.8320 - accuracy: 0.9827 - fn: 5097.3999 - fp: 4526.6157 - loss: 0.0387 - tn: 424980.5938 - tp: 125451.4062\n",
      "Epoch 5: saving model to /opt/artifacts/.weights.h5\n",
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 29ms/step - FN: 2480.0000 - FP: 1360.0000 - TN: 78645.0000 - TP: 77515.0000 - accuracy: 0.9830 - fn: 10105.0000 - fp: 8948.0000 - loss: 0.0381 - tn: 849970.0000 - tp: 250977.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7ff760dfa5a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print (\"Learning...\")\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, save_weights_only=True, verbose=1)\n",
    "train_generate = gen_training_batch(df,mapper,train_indices,batch_size)\n",
    "lstm_model.fit(train_generate, epochs=5, steps_per_epoch=steps_per_epoch, verbose=1, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "w-e4YSqdJBb7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "lstm_model.save(artifact_dir + 'model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5898230  4785713  4951019 ... 13709314 11370319 13741356]\n"
     ]
    }
   ],
   "source": [
    "def create_sample_data_to_insert(df, indices):\n",
    "    print(indices)\n",
    "    rows = indices.shape[0]\n",
    "    index_array = np.zeros((rows, seq_length), dtype=np.int32)\n",
    "    for i in range(seq_length):\n",
    "        index_array[:,i] = indices + 1 - seq_length + i\n",
    "    uniques = np.unique(index_array.flatten())\n",
    "    df.loc[uniques].to_csv(artifact_dir + 'data_to_insert.csv', index_label='Index', header=False)\n",
    "\n",
    "create_sample_data_to_insert(df, validate_indices[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
